<!DOCTYPE HTML>
<!--
	Phantom by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">
		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
					<header id="header">
						<div class="inner">

						</div>
					</header>

				<!-- Main -->
					<div id="main">
						<div class="inner">
							<h1>Data Dimensionality Reduction</h1>
							<span class="image main"><img src="images/8caf2005-c73b-4f60-a522-beb264a0e475.jpeg" alt="" /></span>

                            <p>When implementing machine learning algorithms, the inclusion of more features might lead to worsening performance issues. Increasing the number of features will not always improve classification accuracy, which is also known as the curse of dimensionality. Hence, we apply dimensionality reduction to improve classification accuracy by selecting the optimal set of lower dimensionality features. I have selected a Wine dataset, available at <em>Sklearn</em> to show some techniques of dimensionality reduction.

                            </p>
                            <h2>Feature Selection and Correlation Matrix</h2>
                            <p>The correlation matrix is the first step in dimensionality reduction because it gives an idea of the number of features that strongly relate. If two features strongly correlate between each other, it is likely that one of them can be dropped without negatively affecting the ML model.</p>
                            <iframe src="data_dimensionality_reduction/correlation_matrix.html" width="100%" height="600px" frameborder="0"></iframe>
                            Every feature with a correlation above 0.7 are considered strongly correlated features. For example: 
                            <li>flavanoids / total phenols = 0.86</li>
                            <li>flavanoids / od280/od315_of_diluted_wines = 0.79</li>
                            <li>od280/od315_of_diluted_wines/ total phenols = 0.7 </li>

                            <p>In this case, if I needed to perform a dimensionality reduction I could keep only <em>Flavanoids</em> and drop the other two. I will keep them for now cause I want to show some other graphs.</p>
                            <h2>Spotting Good Features</h2>
                            <p>Imagining you want to train a model to differentiate bewteen <em>labels</em> from classes 1, 2, 3. With the pair plot below you can spot how good each feature is for each of these clusters.</p>
                            <iframe src="data_dimensionality_reduction/spotting_good_features.html" width="100%" height="600px" frameborder="0"></iframe>
                            Some features separate quite well the classes. For example, Proline seems to be a quite explicative variable itself, especially for wine 0 (blue dots). Also color intensity, coupled with total phenols or flavanoids separates quite well the classes.

                            <h2>Principal Component Analysis (PCA)</h2>
                            <p>Principal component analysis, or PCA, is a statistical procedure that allows you to summarize the information content in large data tables by means of a smaller set of “summary indices” that can be more easily visualized and analyzed. It is a common strategy to approach to reduce data dimensionality, which in some cases help to improve the performance of Machine/Deep Learning Algorithms. PCA can be performed using Python, as demonstrated below.</p>							
							<iframe src="data_dimensionality_reduction/pca.html" width="100%" height="600px" frameborder="0"></iframe>
                            Analysing the results of PCA, we can conclude that:
                            <li>The first principal component (PC_1) explains 40.7% of the variance.</li>
                            <li>The second principal component (PC_2) explains 19.0% of the variance.</li>
                            <li>The third principal component (PC_3) explains 8.6% of the variance.</li>
                            <li>The fourth principal component (PC_4) explains 7.4% of the variance.</li>

                            These 4 components together represents <em>75.7%</em> of the dataset, which is very significant and we can simplify the dataset from 13 features to only 4 features. I have implemented a simple SVC classifier to demonstrate the impact of this in ML classification.

                            <h2>Comparing SVC with PCA and Normal Dataset</h2>

                            <iframe src="data_dimensionality_reduction/pca_model.html" width="100%" height="600px" frameborder="0"></iframe>

                            <p>The best model with the complete dataset had an score of <em>0.976</em>, while the best model with the PCA features had a performance of <em>0.968</em>, however, as the number of features is smaller it is less likely that the PCA model is overfitted.</p>
		
					</div>

				<!-- Footer -->
				<footer id="footer">
					<div class="inner">
						<section>
							<h2>Follow</h2>
							<ul class="icons">
								<li><a href="https://github.com/MeliseRocha" class="icon brands style2 fa-github"><span class="label">GitHub</span></a></li>
							</ul>
						</section>
						<ul class="copyright">
							<li>&copy; Untitled. All rights reserved</li>
						</ul>
					</div>
				</footer>

		</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>